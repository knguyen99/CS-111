NAME: Khoi Nguyen
EMAIL: knguyen99@g.ucla.edu
ID: #########

Files:
	lab2_add.c: a C program that implements and tests a shared variable add function, 
	implements specified command line options, and produces specified output statistics.

	lab2_list.c: a Cprogrma that implements and tests a shared Sorted liknked list,
	implements specified command line options, and produces specified output statistics.

	SortedList.h:: a header file (supplied) describing the interfaces for linked 
	list operations.
	
	SortedList.c: implementation of header file, includes insert, delete, lookup and 
	length functions
	
	Makefile: supports build, test, dist, clean, and default build targets
	
	lab2_add.csv: contains data for lab2_add.c testing

	lab2_list.csv: contains data for lab2_list.c testing

	lab2_add-1.png ...threads and iterations required to generate a failure (with and without yields)
	
	lab2_add-2.png … Average time per operation with and without yields.

	lab2_add-3.png … Average time per (single threaded) operation vs. the number of iterations.

	lab2_add-4.png threads and iterations that can run successfully with yields under each of the three synchronization methods.

	lab2_add-5.png Average time per (multi-threaded) operation vs. the number of threads, for all four versions of the add function.

	lab2_list-1.png … average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
	
	lab2_list-2.png … threads and iterations required to generate a failure (with and without yields).
	
	lab2_list-3.png … iterations that can run (protected) without failure.
	
	lab2_list-4.png … (corrected) average time per operation (for unprotected, mutex, and spin-lock) vs. number of threads.

Questions:

	2.1.1) It takes more iterations to see errors because with more iterations,
	it takes longer to create and execute threads, thus later threads can run
	despite the intial thread still trying to finish. This means that the 
	shared variable will be accessed at different values at different threads
	and with no protection, the final counter variable can vary drastically.
	With smaller iterations, threads can finish faster than new threads can 
	be created, and thus there is less of a risk for race conditions, resulting
	in more reliable final counter value. With shorter iterations, the calculation
	in the add funciton is finished before new threads can alter the shared 
	variable.


	2.1.2) Yield runs are much slower because sched_yield is called, thus threads
	relinquish the CPU and other threads get to run. With more context switches, 
	the total time increases since it takes time to switch threads. The additional
	time is going into preparring the context switch, which includes saving and 
	loading registers and removing and replacing stacks and heaps. It is not
	possible because when yielding, the time it takes to switch processes is added
	to the total time. Thus, it is inaccuarate to find the time per operation
	since the time it takes to switch processes is not part of the time of actually
	executing operations.

	2.1.3) The average cost per operation drops drops as increased operations because
	with larger amounts of operations, the time to create and join threads, which is 
	is very costly, is divided by much more iterations and thus the cost per operation
	goes down. If cost per iteration is a function, we want to choose very large 
	iterations such that the cost to create and join threads is insignificant compared
	to the actual computations, thus the cost per iteration is more accurate.

	2.1.4) For low number of threads, operations perform similarly because there is less 
	competition over resources with smaller amounts of threads. With low threads, the
	overall time is spent in thread creation and joining, rather than waiting for resources
	to become available for use. The three protected operations slow down the program
	as threads increase because more threads will have to wait in order to access the 
	critcal section required for their operation. Because of this, as time passes,
	less threads will be able to work concurrently and threads will spend more time waiting. 
	Spin locks are expensice for larger amounts of threads because spin locks use a 
	polling method, which uses continuously uses resources while waiting for the lock.

	2.2.1) In both funcitons, with an increase of threads the cost per operation 
	increases at the a constant exponential rate. With the exponential graph, 
	that means the threads vs cost per operation has a linear slope. Differences
	include that at lower threads, the cost per operation is smaller in the list 
	function. However, for increased threads, the add function seems to level off
	to around 100 ns per operation. As for the list function with more threads,
	there is not enough data to see a correlation like that. Both graphs increase
	linearly in the exponential graph.	

	2.2.2) For the list function, spin locks seem to take a higher toll on cost
	per operation consistently for any amount of threads. For the list function
	spin locks are around 1000 times more nanoseconds per thread than mutex locks.
	As for the add function, the mutex and spin lock protections seem to be 
	around the same points for most of the graph, thus are fairly similar. This 
	could be because the critical sections in the add function do not take as much
	time to complete operations, and there are less critical sections. In the list
	function, both protections have a consistent slope, thus increase at the same
	rate throughout. In the add function, there is a faster change when comparing 
	1 to 2 threads, however the slope begins to level off at around 2 and upward
	threads for mutex locks, while spin locks seem to increase linearly in the 
	graph.
